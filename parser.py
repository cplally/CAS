'''
The parser for CAS
'''

import re
from collections import OrderedDict

## A list of our operators, in order
Operators = ['^', '*', '/', '+', '-']

## An operator precedence comparator
def relative_precedence(op1, op2):
    if Operators.index(op1) < Operators.index(op2):
        return 1
    elif Operators.index(op1) > Operators.index(op2):
        return -1
    else:
        return 0

## An exception we'll need for the tokenizer
class IllegalToken(Exception):
    def __init__(self, error_msg):
        Exception.__init__(self, error_msg)

## ...and one we'll need for the parser
class IllegalExpr(Exception):
    def __init__(self, error_msg):
        Exception.__init__(self, error_msg)

## The parser class -- also contains the tokenizer
class Parser():
    def __init__(self):
        #self.expr_string = string

        self.scanning_table = OrderedDict(
            ((r'[0-9]\.?[0-9]*', Number),
             (r'\-', MinusOp),
             (r'\+|\*|\^|/', Operator),
             (r'\(', OpenParen),
             (r'\)', CloseParen),
             (r'\w+', Name)))
        
        self.operator_precedence = ['^', '*', '/', '+', '-']
    
    # Define the expression-specific parsing routines
    # --------------------------------------------------------------------------
    def __parseFunc(self, expr):
        return ParseTree([Func(expr[0].value),
                          self.parse(expr[2:-1], tokenize=False)])
    
    def __parseParen(self, expr):
        return self.parse(expr[1:-1], tokenize=False)
    
    def __parseNeg(self, expr):
        return ParseTree([expr[0], self.parse(expr[1:], tokenize=False)])
    
    def __parseOp(self, expr):
        return ParseTree([expr[1],
                           expr[0],
                           self.parse(expr[2:], tokenize=False)])
    
    def __parseName(self, expr):
        return ParseTree(expr[0])
    
    def __parseNumber(self, expr):
        return ParseTree(expr[0])
    
    # End expression-specific parsers.
    # On to the general case
    # --------------------------------------------------------------------------
    def parse(self, expr, tokenize=True):
        '''Parse the token stream generated by the tokenizer.'''
        if tokenize:
            # get the token stream, then move on to parsing
            token_stream = self.tokenize(expr)
        
        # first up, let's create patterns defining some expressions to look for
        # in the token stream
        ParenExpr = (OpenParen, Wildcard, CloseParen)
        FuncExpr = (Name, ) + ParenExpr
        NegExpr = (MinusOp, Wildcard)
        OpExpr = (Wildcard, Operator, Wildcard)
        
        # create a preliminary parsing table
        patterns_and_rules = OrderedDict(
            ((FuncExpr, self.__parseFunc),
             (ParenExpr, self.__parseParen),
             (NegExpr, self.__parseNeg),
             (OpExpr, self.__parseOp),
             ((Name,), self.__parseName),
             ((Number,), self.__parseNumber)))
        
        # this sub-function will recursively traverse a given list of tokens,
        # identifying expressions according to the patterns above, parsing them,
        # and sticking the result back in the list -- this way, the entire list
        # will eventually be reduced to one element: a parse tree, ready for
        # use.
        def subparse(tokens):
            if len(tokens) > 1:
                match_found = False
                for p in patterns_and_rules.keys():
                    if len(p) <= len(tokens):
                        m = self.match(p, tokens)
                        if m:
                            match_found = True
                            parsed_expr = patterns_and_rules[p](m.value)
                            return subparse([parsed_expr]+tokens[m.end:])
                if match_found == False:
                    raise IllegalExpr('Illegal expression found -- check input')
            else:
                return tokens[0]
        
        if tokenize:
            return subparse(token_stream)
        else:
            return subparse(expr)

    def tokenize(self, expr_string):
        '''Turn the given string into a stream of tokens.'''
        
        def scan(string, token_stream):
            if string != '':
                match_found = False
                for regex in self.scanning_table.keys():
                    m = re.match(regex, string)
                    if m is not None:
                        match_found = True
                        return scan(string[m.end():],
                                    token_stream +
                                    [self.scanning_table[regex](m.group())])
                # if we loop through all the regexes and still don't have a
                # match, then we have encountered an illegal token
                if match_found == False:
                    raise IllegalToken('Illegal token encountered -- check input')
            else:
                return token_stream
        
        return scan(expr_string, [])
    
    def __termination_cond(self, item, ref_pdepth,
                           current_pdepth, precedence_flag):
        '''Check to see if an expression has naturally terminated'''
        if precedence_flag:
            termination_tokens = ('+', '-')
        else:
            termination_tokens = ('*', '/', '+', '-')
        
        if item.value in termination_tokens and ref_pdepth + current_pdepth == 0:
            return True
        else:
            return False
        
    def __update_pdepth(self, item, pdepth):
        '''Checks if the given item is a Parenthesis token and, if it is,
        updates the parentheses depth given by pdepth'''
        if isinstance(item, OpenParen):
            return (pdepth + 1)
        elif isinstance(item, CloseParen):
            return (pdepth - 1)
        else:
            return pdepth
        
    def match(self, pattern, lst):
        # 1. Take the first element of pattern. Does it match the head of lst?
        # 1a. Yes -- append the element to matches; go back to (1) with the next
        #     pattern.
        # 1b. No -- Is it a wildcard? If so, append elements of lst to matches
        #     (keeping track of paren_depth) until we match the next elt of
        #     pattern. Otherwise, this pattern does not match; return False.
        matches = []
        cursor = 0
        paren_depth = 0
        low_precedence_operator_flag = False
        
        for i in range(0, len(pattern)):
            p = pattern[i]
            if p == Wildcard:
                temp_pdepth = 0
                next_found = False
                for l in lst[cursor:]:
                    temp_pdepth = self.__update_pdepth(l, temp_pdepth)
                    terminate_p =\
                        self.__termination_cond(l, paren_depth,
                                                temp_pdepth,
                                                low_precedence_operator_flag)
                    if i+1 < len(pattern) and isinstance(l, pattern[i+1]) and\
                            temp_pdepth + paren_depth == 0:
                        next_found = True
                        break
                    elif terminate_p:
                        next_found = True
                        break
                    if lst[cursor+1:] == []:
                        next_found = True
                    matches.append(l)
                    cursor += 1
                # if the wildcard-matched expression does not terminate, we have
                # a problem
                if next_found == False:
                    raise IllegalExpr('malformed expression -- check input')
            elif isinstance(lst[cursor], p):
                # the current lst element matches the current pattern element --
                # make a note and keep going
                paren_depth = self.__update_pdepth(lst[cursor], paren_depth)
                matches.append(lst[cursor])
                
                if isinstance(lst[cursor], Operator):
                    if lst[cursor].value in ('+', '-'):
                        low_precedence_operator_flag = True
                    else:
                        low_precedence_operator_flag = False
                cursor += 1
            else:
                # the current lst element does not match the current pattern
                # element -- this pattern does not match; return None
                return None
        return Match(matches, cursor)

class Match():
    '''Match class; nicely encodes the result of our matching algorithm'''
    def __init__(self, matching_seq, end_of_match):
        self.value = matching_seq
        self.end = end_of_match
    def __str__(self):
        return 'Match: %s %d' % ([x.value for x in self.value], self.end)

## The following are classes representing the allowable types of tokens, plus
## the Token base class
## -----------------------------------------------------------------------------

class Token():
    '''Token base class'''
    def __init__(self, token_string=''):
        self.value = token_string
    def __str__(self):
        return self.value

class Number(Token):
    '''Number token'''
    def __init__(self, token_string):
        Token.__init__(self)
        if type(token_string) == str:
            if token_string.find('.') != -1:
                self.value = float(token_string)
            else:
                self.value = int(token_string)
        else:
            self.value = token_string
    def __str__(self):
        return '%s' % self.value

class Operator(Token):
    '''Operator token'''
    def __init__(self, token_string):
        Token.__init__(self, token_string)

class MinusOp(Operator):
    '''
    Minus/negation operator token; needed primarily to deal with negations
    '''
    def __init__(self, token_string):
        Operator.__init__(self, token_string)

class OpenParen(Token):
    '''Open parenthesis token'''
    def __init__(self, token_string):
        Token.__init__(self, token_string)
    
class CloseParen(Token):
    '''Close parenthesis token'''
    def __init__(self, token_string):
        Token.__init__(self, token_string)

class Name(Token):
    '''Name token'''
    def __init__(self, token_string):
        Token.__init__(self, token_string)

class Wildcard(Token):
    '''
    Wildcard token -- needed for type magic by the pattern matching
    machinery
    '''
    def __init__(self):
        Token.__init__(self)

class Func(Name):
    '''Function token; makes parse tree look nicer.'''
    def __init__(self, token_string):
        Name.__init__(self, token_string)

class Transform(Name):
    '''Transform token; used to represent integrals that CAS cannot evaluate.'''
    def __init__(self, token_string=''):
        Name.__init__(self, token_string)

class ParseTree(Token):
    '''
    ParseTree
    
    The parse tree must itself be a token for the parser to work as designed.
    '''
    def __init__(self, value):
        Token.__init__(self)
        self.value = value
        self.root = value[0]
        self.left, self.right = None, None
        
        if len(value) > 1:
            self.left = value[1]
        if len(value) > 2:
            self.right = value[2]
        
        self.children = (self.left, self.right)

    def __str__(self):
        print_version = []
        for x in self.value:
            if isinstance(x, ParseTree):
                print_version.append([str(i) for i in x.value])
            else:
                print_version.append(str(x))
        return str(print_version)
## END -------------------------------------------------------------------------
